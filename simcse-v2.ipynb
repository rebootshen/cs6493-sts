{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from abc import ABC\n",
    "from tqdm.notebook import tqdm\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Union, Optional, Dict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer,BertConfig  #, TrainingArguments, Trainer\n",
    "from transformers.trainer import Trainer,TrainingArguments\n",
    "from transformers.models.bert.modeling_bert import BertPreTrainedModel, BertModel\n",
    "from transformers.tokenization_utils_base import PaddingStrategy, PreTrainedTokenizerBase\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput, BaseModelOutputWithPoolingAndCrossAttentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Set path to SentEval\n",
    "PATH_TO_SENTEVAL = './SentEval'\n",
    "PATH_TO_DATA = './SentEval/data'\n",
    "\n",
    "# Import SentEval\n",
    "sys.path.insert(0, PATH_TO_SENTEVAL)\n",
    "import senteval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model in all STS tasks\n",
    "def print_table(task_names, scores):\n",
    "    tb = PrettyTable()\n",
    "    tb.field_names = task_names\n",
    "    tb.add_row(scores)\n",
    "    print(tb)\n",
    "    \n",
    "def evalModel(model,tokenizer, pooler): \n",
    "    tasks = ['STS12', 'STS13', 'STS14', 'STS15', 'STS16', 'STSBenchmark', 'SICKRelatedness']\n",
    "    \n",
    "    params = {'task_path': PATH_TO_DATA, 'usepytorch': True, 'kfold': 5}\n",
    "    params['classifier'] = {'nhid': 0, 'optim': 'rmsprop', 'batch_size': 128,\n",
    "                                'tenacity': 3, 'epoch_size': 2}\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def prepare(params, samples):\n",
    "        return\n",
    "\n",
    "    def batcher(params, batch, max_length=None):\n",
    "            # Handle rare token encoding issues in the dataset\n",
    "            if len(batch) >= 1 and len(batch[0]) >= 1 and isinstance(batch[0][0], bytes):\n",
    "                batch = [[word.decode('utf-8') for word in s] for s in batch]\n",
    "\n",
    "            sentences = [' '.join(s) for s in batch]\n",
    "            \n",
    "            batch = tokenizer.batch_encode_plus(\n",
    "                sentences,\n",
    "                return_tensors='pt',\n",
    "                padding=True,\n",
    "                max_length=max_length,\n",
    "                truncation=True\n",
    "            )\n",
    "            # Move to the correct device\n",
    "            for k in batch:\n",
    "                batch[k] = batch[k].to(device)\n",
    "            \n",
    "            # Get raw embeddings\n",
    "            with torch.no_grad():\n",
    "                pooler_output = model(**batch, output_hidden_states=True, return_dict=True)\n",
    "                if pooler == \"cls_before_pooler\":\n",
    "                    pooler_output = pooler_output.last_hidden_state[:, 0]\n",
    "                elif pooler == \"cls_after_pooler\":\n",
    "                    pooler_output = pooler_output.pooler_output\n",
    "\n",
    "            return pooler_output.cpu()\n",
    "    results = {}\n",
    "\n",
    "    for task in tasks:\n",
    "        se = senteval.engine.SE(params, batcher, prepare)\n",
    "        result = se.eval(task)\n",
    "        results[task] = result\n",
    "    task_names = []\n",
    "    scores = []\n",
    "    for task in ['STS12', 'STS13', 'STS14', 'STS15', 'STS16', 'STSBenchmark', 'SICKRelatedness']:\n",
    "        task_names.append(task)\n",
    "        if task in results:\n",
    "            if task in ['STS12', 'STS13', 'STS14', 'STS15', 'STS16']:\n",
    "                scores.append(\"%.2f\" % (results[task]['all']['spearman']['all'] * 100))\n",
    "            else:\n",
    "                scores.append(\"%.2f\" % (results[task]['test']['spearman'].correlation * 100))\n",
    "        else:\n",
    "            scores.append(\"0.00\")\n",
    "    task_names.append(\"Avg.\")\n",
    "    scores.append(\"%.2f\" % (sum([float(score) for score in scores]) / len(scores)))\n",
    "    print_table(task_names, scores)\n",
    "\n",
    "    return sum([float(score) for score in scores])/len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataArguments:\n",
    "    train_file: str = field(default=\"./data/wiki1m_for_simcse.txt\",\n",
    "                            metadata={\n",
    "    \"help\": \"The path of train file\"})\n",
    "    model_name_or_path: str = field(default=\"bert-base-uncased\",\n",
    "                                    metadata={\n",
    "    \"help\": \"The name or path of pre-trained language model\"})\n",
    "    max_seq_length: int = field(default=32,\n",
    "                                metadata={\n",
    "    \"help\": \"The maximum total input sequence length after tokenization.\"})\n",
    "\n",
    "#\n",
    "#training_args = TrainingArguments(\n",
    "#        output_dir=\"./checkpoints\",\n",
    "#        num_train_epochs=1,\n",
    "#        per_device_train_batch_size=64,\n",
    "#        learning_rate=3e-5,\n",
    "#        load_best_model_at_end=True,\n",
    "#        overwrite_output_dir=True,\n",
    "#        do_train=True,\n",
    "#        do_eval=False,\n",
    "#        logging_steps=10)\n",
    "training_args = TrainingArguments(\n",
    "        output_dir=\"trainer_models\",\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=64,\n",
    "        per_device_eval_batch_size  = 64,\n",
    "        evaluation_strategy   = \"steps\",\n",
    "        eval_steps            = 125,\n",
    "        learning_rate=3e-5,\n",
    "        load_best_model_at_end=True,\n",
    "        overwrite_output_dir=True,\n",
    "        do_train=True,\n",
    "        do_eval=False, \n",
    "        logging_steps=10)\n",
    "\n",
    "data_args = DataArguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "515cf2fc639a45eeb8d683872b28b569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "YMCA in South Australia\n"
     ]
    }
   ],
   "source": [
    "# 初始化tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(data_args.model_name_or_path)\n",
    "# 读取训练数据\n",
    "with open(data_args.train_file, encoding=\"utf8\") as file:\n",
    "    texts = [line.strip() for line in tqdm(file.readlines())]\n",
    "print(type(texts))\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairDataset(Dataset):\n",
    "    def __init__(self, examples: List[str]):\n",
    "        total = len(examples)\n",
    "        # 将所有样本复制一份用于对比学习\n",
    "        sentences_pair = examples + examples\n",
    "        sent_features = tokenizer(sentences_pair,\n",
    "                                  max_length=data_args.max_seq_length,\n",
    "                                  truncation=True,\n",
    "                                  padding=False)\n",
    "        features = {\n",
    "    }\n",
    "        # 将相同的样本放在同一个列表中\n",
    "        for key in sent_features:\n",
    "            features[key] = [[sent_features[key][i], sent_features[key][i + total]] for i in tqdm(range(total))]\n",
    "        self.input_ids = features[\"input_ids\"]\n",
    "        self.attention_mask = features[\"attention_mask\"]\n",
    "        self.token_type_ids = features[\"token_type_ids\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return {\n",
    "    \n",
    "            \"input_ids\": self.input_ids[item],\n",
    "            \"attention_mask\": self.attention_mask[item],\n",
    "            \"token_type_ids\": self.token_type_ids[item]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3559d51549941eea6f14b7d92386a9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b436e61ba1b42509733c07a3b841309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9811c2df5cba4871b2bc29dfc72f91e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 26866, 1999, 2148, 2660, 102], [101, 26866, 1999, 2148, 2660, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]], 'token_type_ids': [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]]}\n"
     ]
    }
   ],
   "source": [
    "#train_dataset1 = PairDataset(texts)\n",
    "train_dataset = PairDataset(texts[:10000])\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle\n",
    "\n",
    "#with open(\"./data/train_dataset\", \"wb\") as fp2:   #Pickling\n",
    "#    pickle.dump(train_dataset1, fp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"./data/train_dataset\", \"rb\") as fp2:   # Unpickling\n",
    "#    train_dataset = pickle.load(fp2)\n",
    "#print(train_dataset[0])\n",
    "#print(len(train_dataset))\n",
    "#print(type(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 1996, 7328, 1997, 9569, 7490, 2964, 1010, 11295, 4676, 1998, 2969, 2128, 15204, 2102, 3754, 5171, 1997, 2049, 8759, 2018, 2445, 2019, 5020, 25691, 28126, 2000, 2148, 2827, 3241, 2013, 102], [101, 1996, 7328, 1997, 9569, 7490, 2964, 1010, 11295, 4676, 1998, 2969, 2128, 15204, 2102, 3754, 5171, 1997, 2049, 8759, 2018, 2445, 2019, 5020, 25691, 28126, 2000, 2148, 2827, 3241, 2013, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollator:\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, List[int]]]) -> Dict[str, torch.Tensor]:\n",
    "        special_keys = ['input_ids', 'attention_mask', 'token_type_ids']\n",
    "        batch_size = len(features)\n",
    "        if batch_size == 0:\n",
    "            return\n",
    "        # flat_features: [sen1, sen1, sen2, sen2, ...]\n",
    "        flat_features = []\n",
    "        for feature in features:\n",
    "            for i in range(2):\n",
    "                flat_features.append({\n",
    "    k: feature[k][i] for k in feature.keys() if k in special_keys})\n",
    "        # padding\n",
    "        batch = self.tokenizer.pad(\n",
    "            flat_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        # batch_size, 2, seq_len\n",
    "        batch = {\n",
    "    k: batch[k].view(batch_size, 2, -1) for k in batch if k in special_keys}\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'token_type_ids'])\n",
      "torch.Size([32, 2, 32])\n"
     ]
    }
   ],
   "source": [
    "collate_fn = DataCollator(tokenizer)\n",
    "\n",
    "\n",
    "#dataloader = DataLoader(train_dataset, batch_size=4, collate_fn=collate_fn)\n",
    "dataloader = DataLoader(train_dataset, batch_size=32, collate_fn=collate_fn)\n",
    "\n",
    "batch = next(iter(dataloader))\n",
    "print(batch.keys())\n",
    "print(batch[\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForCL: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForCL from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForCL from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForCL were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['mlp.dense.bias', 'mlp.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['loss', 'logits'])\n"
     ]
    }
   ],
   "source": [
    "# 全连接层，用于投影CLS的向量表示\n",
    "class MLPLayer(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(input_size, output_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        x = self.dense(features)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "# 相似度层，计算向量间相似度\n",
    "class Similarity(nn.Module):\n",
    "    def __init__(self, temp):\n",
    "        super().__init__()\n",
    "        self.temp = temp\n",
    "        self.cos = nn.CosineSimilarity(dim=-1)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        return self.cos(x, y) / self.temp\n",
    "\n",
    "    \n",
    "# SimCSE的完整模型结构\n",
    "class BertForCL(BertPreTrainedModel, ABC):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.bert = BertModel(config)\n",
    "        self.mlp = MLPLayer(config.hidden_size, config.hidden_size)\n",
    "        self.sim = Similarity(temp=0.05)\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids=None,\n",
    "                attention_mask=None,\n",
    "                token_type_ids=None,\n",
    "                position_ids=None,\n",
    "                head_mask=None,\n",
    "                inputs_embeds=None,\n",
    "                labels=None,\n",
    "                output_attentions=None,\n",
    "                output_hidden_states=None,\n",
    "                return_dict=None,\n",
    "                sent_emb=False):\n",
    "        if sent_emb:\n",
    "            # 模型推断时使用的forward\n",
    "            return self.sentemb_forward(input_ids=input_ids,\n",
    "                                        attention_mask=attention_mask,\n",
    "                                        token_type_ids=token_type_ids,\n",
    "                                        position_ids=position_ids,\n",
    "                                        head_mask=head_mask,\n",
    "                                        inputs_embeds=inputs_embeds,\n",
    "                                        labels=labels,\n",
    "                                        output_attentions=output_attentions,\n",
    "                                        output_hidden_states=output_hidden_states,\n",
    "                                        return_dict=return_dict)\n",
    "        else:\n",
    "            # 模型训练时使用的forward\n",
    "            return self.cl_forward(input_ids=input_ids,\n",
    "                                   attention_mask=attention_mask,\n",
    "                                   token_type_ids=token_type_ids,\n",
    "                                   position_ids=position_ids,\n",
    "                                   head_mask=head_mask,\n",
    "                                   inputs_embeds=inputs_embeds,\n",
    "                                   labels=labels,\n",
    "                                   output_attentions=output_attentions,\n",
    "                                   output_hidden_states=output_hidden_states,\n",
    "                                   return_dict=return_dict)\n",
    "\n",
    "    def sentemb_forward(self,\n",
    "                        input_ids=None,\n",
    "                        attention_mask=None,\n",
    "                        token_type_ids=None,\n",
    "                        position_ids=None,\n",
    "                        head_mask=None,\n",
    "                        inputs_embeds=None,\n",
    "                        labels=None,\n",
    "                        output_attentions=None,\n",
    "                        output_hidden_states=None,\n",
    "                        return_dict=None):\n",
    "        # 1.使用bert进行编码\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "        # 2.取cls的表示\n",
    "        cls_output = outputs.last_hidden_state[:, 0]\n",
    "        # 3.使用MLP进行投影\n",
    "        cls_output = self.mlp(cls_output)\n",
    "        # 返回\n",
    "        if not return_dict:\n",
    "            return (outputs[0], cls_output) + outputs[2:]\n",
    "\n",
    "        return BaseModelOutputWithPoolingAndCrossAttentions(\n",
    "            pooler_output=cls_output,\n",
    "            last_hidden_state=outputs.last_hidden_state,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "        )\n",
    "\n",
    "    def cl_forward(self,\n",
    "                   input_ids=None,\n",
    "                   attention_mask=None,\n",
    "                   token_type_ids=None,\n",
    "                   position_ids=None,\n",
    "                   head_mask=None,\n",
    "                   inputs_embeds=None,\n",
    "                   labels=None,\n",
    "                   output_attentions=None,\n",
    "                   output_hidden_states=None,\n",
    "                   return_dict=None):\n",
    "        # input_ids: batch_size, num_sent, len\n",
    "        batch_size = input_ids.size(0)\n",
    "        num_sent = input_ids.size(1)  # 2\n",
    "        # 1. 重塑输入张量的形状，使其满足bert对输入的要求\n",
    "        # input_ids: batch_size * num_sent, len\n",
    "        input_ids = input_ids.view((-1, input_ids.size(-1)))\n",
    "        attention_mask = attention_mask.view((-1, attention_mask.size(-1)))\n",
    "        # 2. 使用bert进行编码\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "        # 3. 取cls的向量表示\n",
    "        cls_output = outputs.last_hidden_state[:, 0]\n",
    "        # 4. 重塑形状\n",
    "        cls_output = cls_output.view((batch_size, num_sent, cls_output.size(-1)))\n",
    "        # 5. 全连接层投影\n",
    "        # batch_size, num_sent, 768\n",
    "        cls_output = self.mlp(cls_output)\n",
    "        # 6. 将同一批样本的两次向量表示分开\n",
    "        z1, z2 = cls_output[:, 0], cls_output[:, 1]\n",
    "        # 7. 计算两两相似度，得到相似度矩阵cos_sim\n",
    "        cos_sim = self.sim(z1.unsqueeze(1), z2.unsqueeze(0))\n",
    "        # 8. 生成标签[0,1,...,batch_size-1]，该标签用于提高相似度句子cos_sim对角线，并降低非对角线\n",
    "        labels = torch.arange(cos_sim.size(0)).long().to(self.device)\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(cos_sim, labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (cos_sim,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=cos_sim,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "model = BertForCL.from_pretrained(data_args.model_name_or_path)\n",
    "cl_out = model(**batch, return_dict=True)\n",
    "print(cl_out.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimCSEModel(BertPreTrainedModel):\n",
    "    def __init__(self, config,temp=0.05):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.mlp = MLPLayer(config.hidden_size, config.hidden_size)\n",
    "        self.sim = Similarity(temp)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        sent_emb=False, # if true, return sentence embedding for evaluation\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\n",
    "            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n",
    "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        if sent_emb:\n",
    "            # return sentence embedding for evaluation\n",
    "            \n",
    "            outputs = self.bert(\n",
    "                input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                position_ids=position_ids,\n",
    "                head_mask=head_mask,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "            )\n",
    "\n",
    "            return outputs\n",
    "        \n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        # view_0 & view_1 are the same sentence go through bert twice\n",
    "        outputs_view_0 = self.bert(\n",
    "            input_ids[:,0],\n",
    "            attention_mask=attention_mask[:,0],\n",
    "            token_type_ids=token_type_ids[:,0],\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        \n",
    "        outputs_view_1 = self.bert(\n",
    "            input_ids[:,1],\n",
    "            attention_mask=attention_mask[:,1],\n",
    "            token_type_ids=token_type_ids[:,1],\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        \n",
    "        # go through mlp twice\n",
    "        pooled_output_0 = self.mlp(outputs_view_0.last_hidden_state[:, 0])\n",
    "        pooled_output_1 = self.mlp(outputs_view_1.last_hidden_state[:, 0])\n",
    "\n",
    "        sim_matrix = self.sim(pooled_output_0.unsqueeze(1), pooled_output_1.unsqueeze(0))\n",
    "        \n",
    "        # labels = [0,1,...,batch_size-1]\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        labels = torch.arange(0,sim_matrix.shape[0],step=1).to(device) #.cuda()\n",
    "        \n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(sim_matrix,labels)\n",
    "        \n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=sim_matrix,\n",
    "            hidden_states=[pooled_output_0,pooled_output_1],\n",
    "            attentions=None,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# override the evaluate method\n",
    "class SimCSETrainer(Trainer):\n",
    "    def __init__(self,**paraments):\n",
    "        super().__init__(**paraments)\n",
    "        \n",
    "        self.best_sts = 0.0\n",
    "        \n",
    "    def evaluate(\n",
    "        self,\n",
    "        eval_dataset: Optional[Dataset] = None,\n",
    "        ignore_keys: Optional[List[str]] = None,\n",
    "        metric_key_prefix: str = \"eval\",\n",
    "        eval_senteval_transfer: bool = False,\n",
    "    ) -> Dict[str, float]:\n",
    "\n",
    "        # SentEval prepare and batcher\n",
    "        def prepare(params, samples):\n",
    "            return\n",
    "\n",
    "        def batcher(params, batch):\n",
    "            sentences = [' '.join(s) for s in batch]\n",
    "            batch = self.tokenizer.batch_encode_plus(\n",
    "                sentences,\n",
    "                return_tensors='pt',\n",
    "                padding=True,\n",
    "            )\n",
    "            for k in batch:\n",
    "                batch[k] = batch[k].to(self.args.device)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**batch, output_hidden_states=True, return_dict=True, sent_emb=True)\n",
    "                pooler_output = outputs.last_hidden_state[:, 0]\n",
    "            return pooler_output.cpu()\n",
    "\n",
    "        # Set params for SentEval (fastmode)\n",
    "        params = {'task_path': PATH_TO_DATA, 'usepytorch': True, 'kfold': 5}\n",
    "        params['classifier'] = {'nhid': 0, 'optim': 'rmsprop', 'batch_size': 128,\n",
    "                                            'tenacity': 3, 'epoch_size': 2}\n",
    "\n",
    "        se = senteval.engine.SE(params, batcher, prepare)\n",
    "        tasks = ['STSBenchmark', 'SICKRelatedness']\n",
    "        self.model.eval()\n",
    "        results = se.eval(tasks)\n",
    "        \n",
    "        stsb_spearman = results['STSBenchmark']['dev']['spearman'][0]\n",
    "        sickr_spearman = results['SICKRelatedness']['dev']['spearman'][0]\n",
    "\n",
    "        metrics = {\"eval_stsb_spearman\": stsb_spearman, \"eval_sickr_spearman\": sickr_spearman, \"eval_avg_sts\": (stsb_spearman + sickr_spearman) / 2} \n",
    "        print(metrics)\n",
    "        \n",
    "        # save and eval model\n",
    "        if metrics[\"eval_avg_sts\"]>self.best_sts:\n",
    "            self.best_sts = metrics[\"eval_avg_sts\"]\n",
    "            evalModel(self.model.bert,tokenizer, pooler = 'cls_before_pooler')\n",
    "            self.save_model(self.args.output_dir+\"/best-model\")\n",
    "            \n",
    "        self.log(metrics)\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))\n",
    "trainer = SimCSETrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_stsb_spearman': 0.3173403740863298, 'eval_sickr_spearman': 0.4457171899435432, 'eval_avg_sts': 0.3815287820149365}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "./SentEval\\senteval\\sts.py:42: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  sent1 = np.array([s.split() for s in sent1])[not_empty_idx]\n",
      "./SentEval\\senteval\\sts.py:43: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  sent2 = np.array([s.split() for s in sent2])[not_empty_idx]\n",
      "Saving model checkpoint to trainer_models/best-model\n",
      "Configuration saved in trainer_models/best-model\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+-------+-------+--------------+-----------------+-------+\n",
      "| STS12 | STS13 | STS14 | STS15 | STS16 | STSBenchmark | SICKRelatedness |  Avg. |\n",
      "+-------+-------+-------+-------+-------+--------------+-----------------+-------+\n",
      "| 21.54 | 32.11 | 21.28 | 37.89 | 44.24 |    20.29     |      42.42      | 31.40 |\n",
      "+-------+-------+-------+-------+-------+--------------+-----------------+-------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in trainer_models/best-model\\pytorch_model.bin\n",
      "tokenizer config file saved in trainer_models/best-model\\tokenizer_config.json\n",
      "Special tokens file saved in trainer_models/best-model\\special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_stsb_spearman': 0.3173403740863298,\n",
       " 'eval_sickr_spearman': 0.4457171899435432,\n",
       " 'eval_avg_sts': 0.3815287820149365}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from filelock import FileLock\n",
    "from prettytable import PrettyTable\n",
    "#from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "# test the evaluate method and see what are the initial results\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Tools\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 10000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 157\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='157' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [157/157 1:41:21, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_stsb_spearman': 0.5441514217880491, 'eval_sickr_spearman': 0.5379106316922329, 'eval_avg_sts': 0.541031026740141}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to trainer_models/best-model\n",
      "Configuration saved in trainer_models/best-model\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+-------+-------+--------------+-----------------+-------+\n",
      "| STS12 | STS13 | STS14 | STS15 | STS16 | STSBenchmark | SICKRelatedness |  Avg. |\n",
      "+-------+-------+-------+-------+-------+--------------+-----------------+-------+\n",
      "| 36.70 | 61.09 | 45.99 | 60.20 | 64.61 |    41.58     |      51.67      | 51.69 |\n",
      "+-------+-------+-------+-------+-------+--------------+-----------------+-------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in trainer_models/best-model\\pytorch_model.bin\n",
      "tokenizer config file saved in trainer_models/best-model\\tokenizer_config.json\n",
      "Special tokens file saved in trainer_models/best-model\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to trainer_models/final\n",
      "Configuration saved in trainer_models/final\\config.json\n",
      "Model weights saved in trainer_models/final\\pytorch_model.bin\n",
      "tokenizer config file saved in trainer_models/final\\tokenizer_config.json\n",
      "Special tokens file saved in trainer_models/final\\special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "#model.resize_token_embeddings(len(tokenizer))\n",
    "#trainer = Trainer(model=model,\n",
    "#                  train_dataset=train_dataset,\n",
    "#                  args=training_args,\n",
    "#                  tokenizer=tokenizer,\n",
    "#                  data_collator=collate_fn)\n",
    "trainer.train()\n",
    "trainer.save_model(\"trainer_models/final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file trainer_models/best-model\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForCL\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file trainer_models/best-model\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForCL.\n",
      "\n",
      "All the weights of BertForCL were initialized from the model checkpoint at trainer_models/best-model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForCL for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "config = BertConfig()\n",
    "\n",
    "#model = SimCSEModel(config).from_pretrained(\"trainer_models/best-model\").to(device)\n",
    "model = BertForCL(config).from_pretrained(\"trainer_models/best-model\").to(device)\n",
    "\n",
    "avg = evalModel(model.bert,tokenizer, pooler = 'cls_before_pooler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
